defaults:
  - logger: tensorboard
  # - callbacks: [early_stopping, sparseml]

_target_: pytorch_lightning.Trainer
# ? set in callbacks
enable_checkpointing: True
# callbacks: null
default_root_dir: null
# ? amp unscale
gradient_clip_val: 1.0
num_nodes: 1
devices: 2
auto_select_gpus: False
tpu_cores: null
overfit_batches: 0.0
track_grad_norm: -1
check_val_every_n_epoch: 1
fast_dev_run: False
accumulate_grad_batches: 1
max_epochs: 20
min_epochs: 1
max_steps: null
min_steps: null
max_time: null
limit_train_batches: 1.0
limit_val_batches: 1.0
limit_test_batches: 1.0
val_check_interval: 1.0
log_every_n_steps: 50
strategy: ddp
# strategy: deepspeed
accelerator: self.reset_train_dataloader
sync_batchnorm: False
precision: 32
num_sanity_val_steps: 2
profiler: null
benchmark: False
# ? reproductivity
deterministic: False
auto_lr_find: False
replace_sampler_ddp: True
auto_scale_batch_size: False
plugins: null
# ? apex
amp_backend: 'native'
move_metrics_to_cpu: False

# callbacks: modelcheckpoint; tqdm, logger
# trainer.fit(ckpt_path="some/path/to/my_checkpoint.ckpt")